Pricing
-------

c5.xlarge - 0.17 / hr
m5.xlarge - 0.192 / hr

Fargate (per hr): 0.04048 per vCPU, 0.004445 per GB memory

4 vCPU / 8GB memory = 0.16192 + 0.03556 = 0.19748
4 vCPU / 16GB memory = 0.16192 + 0.07112 = 0.23304


CPU
---

VM can handle more concurrent clients than containers with same number of CPUS.
Rubis used 1 vCPU and 2G memory.

Container:
    Intel(R) Xeon(R) CPU E5-2686 v4 (2300 MHz)
    1042.0338s sysbench time
    

VM (c5):
    Intel(R) Xeon(R) Platinum 8124M CPU (3400 MHz)
    779.7900s sysbench time
    

VM (m5):
    Intel(R) Xeon(R) Platinum 8175M CPU (3100 MHz)
    855.5076s sysbench time

- Spikes in CPU utilization during sysbench memory/disk benchmarks could be caused by a remote filesystem


Memory
------

Container:
    48777.17 MiB/sec read
    26652.34 MiB/sec write
    15.96 GB idle (with 16 GB / limited to 8 GB)

VM (c5):
    64560.22 MiB/sec read
    29371.23 MiB/sec write
    7.26 GB idle (with 8 GB)

VM (m5):
    60496.50 MiB/sec read
    25515.71 MiB/sec write
    15.38 GB idle (with 16 GB)

- The container reports 16 GB even though it has a hard/soft limit of 8 GB (probably reports the machine it runs on)


Disk
----

Container:
    89.72 MiB/s seq read
    130.77 MiB/s rand read
    89.47 MiB/s seq write
    47.50 MiB/s rand write
    20GB maximum ephemeral storage (must use EFS otherwise)

VM (m5) - 24 GB filesize:
    128.14 MiB/s seq read
    113.45 MiB/s rand read
    128.20 MiB/s seq write
    46.58 MiB/s rand write

VM (c5):
    127.55 MiB/s seq read
    105.62 MiB/s rand read
    128.02 MiB/s seq write
    46.57 MiB/s rand write
    EBS gp2 SSD

VM (m5) - 12 GB filesize:
    11982.47 MiB/s seq read
    11495.88 MiB/s rand read
    128.27 MiB/s seq write
    46.74 MiB/s rand write
    EBS gp2 SSD


- The container has worse seq than rand reads. Confirmed this with a second run. This could be due to remote filesystem.
- Disk _reads_ are much faster when memory size is greater than filesize


Startup Time
------------

Disk usage on the VM was about ~8G, but total disk size was 40G.

Fargate will pull images every time, which explains the long startup time.
Fargate still seems to boot a new instance (~30s) for each container.

1) EC2 (m5.xlarge, 40G disk, gbmperf ami, average of 3 runs): 37s
2a) EC2 (t2.small, 8G disk, base ubuntu ami, average of 3 runs): 22s
2b) Rubis (t2.small, 8G disk): 30s

3) ECS (1.53G image on dockerhub, fargate 1.4, gbmperf, average of 3 runs): 117.5s
4) ECS (gbmperf on ECR): 137s
5) ECS (4C/16G, small image - alpine:latest): 30s
6) ECS (1C/2G, small image): 41s


Data for (2)

i-014fa98282b3f705b
00:26:54
00:27:16

i-0993cd46b5d3def1c
00:31:09
00:31:31

i-06009f1a2c4fc1c05
00:34:23
04:34:46


Data for (3)
arn:aws:ecs:us-east-1:589848030355:task/b6bb8df8-62fb-422f-8119-c719e3ab5fb2
2020-05-03 18:37:28
2020-05-03T18:39:30

arn:aws:ecs:us-east-1:589848030355:task/74784236-b188-49f9-bd3b-640c73fcd1d4
2020-05-03 18:37:29
2020-05-03T18:39:22

arn:aws:ecs:us-east-1:589848030355:task/8ef0a7e8-9613-4e50-a5d8-915b7dc884d8
2020-05-03 18:37:30
2020-05-03T18:39:27


Data for (4)
Start  2020-05-03 23:17:14
d128  2020-05-03 23:19:30

Start  2020-05-03 23:26:12
18d2  2020-05-03 23:28:29
c7d0  2020-05-03 23:28:30


Data for (5)
Start  2020-05-03 22:02:52
05d6  2020-05-03 22:03:20

Start  2020-05-03 22:04:11
219a  2020-05-03 22:04:41
44f1  2020-05-03 22:04:42


Data for (6)
Start  2020-05-03 22:19:01
e294  2020-05-03 22:19:34

Start  2020-05-03 22:20:56
0b8c  2020-05-03 22:21:44
9af4  2020-05-03 22:21:39
