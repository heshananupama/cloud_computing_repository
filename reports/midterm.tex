\documentclass[11pt]{article}

\def\baselinestretch{1}
\usepackage{times}
\usepackage{titlesec}
\usepackage{fancyhdr}  % for displaying head/foot meta info 
\usepackage{pagecounting}
\usepackage{color}
\usepackage[yyyymmdd,hhmmss]{datetime}  % for using currenttime command 
\usepackage{hyperref}
\usepackage{lastpage}
\usepackage{lipsum}

\newcommand\bb[1]{\mbox{\em #1}}
\newcommand{\eat}[1]{}
\newcommand{\hsp}{\hspace*{\parindent}}
\definecolor{gray}{rgb}{0.4,0.4,0.4}


\titleformat{\section}{\vspace{- 0.5 \baselineskip}\normalfont\fontsize{12}{12}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\vspace{- 0.5 \baselineskip}\normalfont\fontsize{11}{11}\bfseries}{\thesubsection}{1em}{}

\begin{document}

\renewcommand{\headrulewidth}{0pt} 
\renewcommand{\footrulewidth}{0pt} 
\pagestyle{fancy}
\cfoot{}
\lhead{}
\rhead{}
\rfoot{\itshape\textcolor{gray}{Page \thepage\ of \pageref{LastPage}}}
\lfoot{\itshape\textcolor{gray}{CS525T Cloud Computing Paper Review}}

%%% Fill in the paper information %%%
\begin{center}
{\LARGE \bf Midterm Review: Comparison Between Virtual Machines and Containers for Cloud End Users} \\
{\normalsize \emph{Heshan Perera, Michael Ludwig}}\\

\end{center}

\section{Project Log}

So far our project is on track, with some changes to our selection of benchmarks. We tried moving forward with MLPerf, however the implementations are all written for GPUs. Although both AWS EC2 and ECS support instances with GPUs, they are prohibitively expensive to work with while using the AWS Educate accounts. Instead, we ended up using a benchmark called gbmperf, which trains a model using three different gradient boosting libraries. This gives a realistic workload for standard ML use cases. We were able to move forward with the RUBiS benchmark as we had planned. 

An additional step we would like to add to our project is using sysbench or another similar tool to benchmark specific resources in order to see which, if any, have different performance. This will help explain any disparities in the other benchmark results.

\section{Complete Work}

For our first phase we had selected two benchmarks, gbmperf and RUBiS. RUBiS is an open-source auction site prototype modeled after eBay.com developed by Rice University. It has been widely used as a benchmark in the area of network research. The workload generated for RUBiS is a HTTP workload where the system will continuosly make requests to the auction site for a dedicated period of time. There are different implementations of the benchmark and the one that we chosen provide information related to the experiemnet in HTML tabular format. In the case of gbperf....... (may be its better to define in a statement or two about gbperf) 

We made sure to have the above benchmarks running in AWS on both VMs and containers. We created AWS Educate accounts to do this. In order to run the benchmarks consistently over several runs, we created virtual machine images (AMIs) and Docker images. The gbmperf source code had an existing Dockerfile to create the image, however some fixes needed to be made to get it to run smoothly. In the case of RUBiS, we needed to create the Dockerfile from the scratch. As it needed to manage multiple services like apache, MySQL we were thinking to use supervisord. However, upon closely working on the problem, we managed to do everything in a simple manner with the help of a shell script. To run on an EC2 instance, the installation commands needed to be adapted, such as to install R, MySQL and other dependencies needed from both benchmarks. In order to run gbmperf on startup, it was run as a systemd service. This also makes it easy to restart for multiple runs. However, Since ECS handles starting containers, nothing was needed to be done for the gbmperf and RUBiS Docker images.

For logging CPU and memory utilization, ECS sends these metrics automatically to AWS CloudWatch. In the case of EC2 instances, you have to make sure to allow the EC2 instance to the CloudWatch upon creation of the instance. However, the metrics did only include a CPU utilization and a separate process needed to be installed to send the metrics like memory utilization to CloudWatch. This is done via a collectd plugin, which was configured to send this data every five seconds. After running the benchmarks a few times on both VMs and containers, we were able to download the metrics using a utility called CloudWatch-dump. This required a modification to download fine-grained periods of data (ten seconds).

For evaluating gbmperf, the benchmark was run five times each on VMs and containers. The CPU and memory utilization was averaged across the runs, and graphed over time to compare the two. Although almost identical across runs, the total runtime and model accuracy was also averaged for comparison.

(include results here)
(explanation/discussion on results)?

In the case of RUBiS, the benchmark was run for three times. Same as for the gbmperf, CPU and memory utilization was averaged and graphed. As the benchmark runs longer than the gbmperf, we set the period to 5 minutes as we tried to retrieve the metrics with CloudWatch-dump. 

(RUBiS CPU/Memory result)
(explanation/discussion on results)?

The workload generated for RUBiS is a HTTP workload where the system will continuously make HTTP requests over 40 minutes of time. RUBiS benchmark provide some results like the number of requests made for different pages of the RUBiS system, and finally provides a summary which includes the metrics like total requests made and average requests made per a second. 

(results given by benchmark itself)
(explanation/discussion on results)?

\section{Unfinished Work}

The main work to complete is a second round of benchmarking and results. This involves installing the benchmarks in both VM and Docker images, running the benchmarks multiple times, and then compiling and polishing the results for presenting. The bulk of time will likely come from building the images and summarizing the results.

After comparing benchmark results, we would like to explore reasons why differences exist, or confirm that they don't, using a tools such as sysbench. This will involve profiling the CPU, memory, and disk of the VMs and containers and comparing the results.

\section{Major Barriers}

So far no major barriers have been blocking us, however certain parts proved challenging. Installing RUBiS and getting it to run was difficult due to lack of documentation and the number of services it depends on to run. We needed to install MySQL, Apache, Java, and more into a single container and automate the setup.

Additionally, to download metric data from AWS involved using a 3rd party utility that required modifications to get the correct fidelity of data. Instrumentation to the EC2 instances was also required to the log CPU and memory.

\hspace{16pt}

[1] https://github.com/szilard/GBM-perf
[2] https://github.com/mogproject/cloudwatch-dump


\end{document}
