\documentclass[11pt]{article}

\def\baselinestretch{1}
\usepackage{times}
\usepackage{titlesec}
\usepackage{fancyhdr} 
\usepackage{pagecounting}
\usepackage{color}
\usepackage[yyyymmdd,hhmmss]{datetime}
\usepackage{hyperref}
\usepackage{lastpage}
\usepackage{lipsum}

\newcommand\bb[1]{\mbox{\em #1}}
\newcommand{\eat}[1]{}
\newcommand{\hsp}{\hspace*{\parindent}}
\definecolor{gray}{rgb}{0.4,0.4,0.4}

\titleformat{\section}{\vspace{- 0.5 \baselineskip}\normalfont\fontsize{12}{12}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\vspace{- 0.5 \baselineskip}\normalfont\fontsize{11}{11}\bfseries}{\thesubsection}{1em}{}

\begin{document}

\renewcommand{\headrulewidth}{0pt} 
\renewcommand{\footrulewidth}{0pt} 
\pagestyle{fancy}
\cfoot{}
\lhead{}
\rhead{}
\rfoot{\itshape\textcolor{gray}{Page \thepage\ of \pageref{LastPage}}}
\lfoot{\itshape\textcolor{gray}{CS525T Cloud Computing Final Report}}

\begin{center}
{\LARGE \bf Project Final Report: Comparison Between Virtual Machines and Containers for Cloud End Users} \\
{\normalsize \emph{Heshan Perera, Michael Ludwig}}\\
\end{center}

% 1. The final report should be about 10-12 pages long using the same Latex template for paper review.
% 2. It is okay to reuse texts from your proposal, but the final report should focus on the following questions. One way to think about the final report is to use the proposal as a roadmap and fill in with information including what you chose to do, and why you chose to do so.
%     a. A concrete description of the problem statement: what were the exact research questions you work on?
%     b. A more comprehensive related work: this is what makes
%     c. A detailed description of the proposed ideas, and techniques that were used to implement them. You should focus on explaining the ideas and rationales, instead of only describing what you have done. It is not necessary to copy and paste the project code as the entire project code should be submitted together and I will have access to them.
%     d. The evaluation section can be structured as 1) experiment setup, 2) experiment results, e.g., tables or figures, 3) result analysis that explains the significance of results, for each question you are evaluating.
%     e. The conclusion section which should provide readers the key takeaways and describe any other solutions that were attempted and potential solutions.
%     f. The division of work done by each team member.


\section{Problem Statement}

Modern cloud computing offers a vast array of options when deciding how to run an application. The traditional approach to this is to use VMs. This requires installing the application on an OS and exporting the entire disk image. When it comes time to run the application, or scale an existing setup, a new VM would be booted from this image. This is perfectly acceptable for services with a consistent workload, but for services such as web applications, databases, or anything where users come and go, or jobs run on a periodic schedule, the workload will likely fluctuate. This means it's important to scale the infrastructure up and down to get the best utilization, decreasing cost for the cloud user, and increasing effective capacity for the cloud provider. To do this, the isolation mechanism needs to be able to start up quickly. Additionally, it needs to have little overhead, so the hardware can be used to its full capacity.

This report aims to identify specific differences and similarities between VMs and containers when using them as an end user on a cloud service, specifically AWS. To start off, it's important to know the basics of each, such as unit cost per hardware resource and startup times. However, the main subject of interest is performance under various workloads. How these technologies fair under common workload scenarios it an important deciding factor when choosing infrastructure. In addition to this, it also helps to pinpoint the cause of performance differences. This may help mitigate these problems for users who don't have much of a choice. Lastly, other factors besides cost and performance may be important for some users, such as level of isolation, so this is also investigated.


\section{Related Work}

Other work exists that compares VMs with containers. "Containers and Virtual Machines at Scale: A Comparative Study" by Chaufournier et al compares VMs and containers on bare metal using five different workloads. They concluded that containers running with LXC have similar performance in most areas except disk I/O, but VMs provide better isolation. "An Updated Performance Comparison of Virtual Machines and Linux Containers" by Felter et al confirms the finding that containers have only a small performance cost versus bare metal. This paper used Docker instead of LXC, which closer aligns to our work due to the packaging format and runtime. Similarly, our work ran benchmarks on VMs and containers, though it was not necessarily on bare-metal. Instead, we used cloud services as presented to customers to compare the two, factoring in cost and scalability. This body of work shows that the two technologies are expected to have similar raw performance in terms of individual resources such as CPU, memory, disk, and network.

One paper that had very similar goals and methods as ours was "Performance Comparison between Container-based and VM-based Services" by Salah et al. They compared the throughput, latency, and CPU utilization of an Apache web service running on EC2 and ECS, and concluded that EC2 has significantly higher performance. However, there are some important differences with our work. Their paper was published in 2017, and used self-managed instances for ECS. We used ECS with Fargate 1.4 which uses containerd on top of EC2, with the new Fargate control-plane that uses Firecracker micro VMs, leading to better security and likely a different performance profile. Additionally, they did not investigate causes for the disparity, such as benchmarking specific resources.

"Container and Microservice Driven Design for Cloud Infrastructure DevOps" by Kang et al does a comparison of VM and container approaches for DevOps. The key takeaway here is the ability of each to scale. They measure the time and resources it takes to deploy and scale parts of an OpenStack control plane. It takes the VM much longer to start up, and slightly longer to instantiate the service, taking 1.5 times longer in total than the container, along with higher CPU usage. They use KVM as the hypervisor and Docker for managing containers.

Other work illustrates the benefits of containers besides scalability and performance, namely easier management of the cluster. "Systems Modeling: Methodologies and Tools" explains the advantages of a container orchestration, including scheduling, fault tolerance, load balancing, configuration management, and service discovery. Although some of these features can be found for VMs, due to the history of microservices, the systems are more accessible for end-users with containers. Kubernetes, Docker Swarm, and others allow automatic management of a production ready system after defining a handful of configurations. The point here is that, in many cases, using containers would be ideal if not outweighted by any possible performance (or security) disadvantages. This report will shed light on that, and can be used to help cloud users determine which technology is right for them.

In terms of security, containers are often considered less secure, in large part due to kernel sharing. "A Measurement Study on Linux Container Security: Attacks and
Countermeasures" by Lin et al confirms this by running 223 exploits on containers and finding that 11 of them break container isolation. They find that the container runtime doesn't provide enough security, as over 50\% of 88 exploits they tested in-depth ran on both the host and inside the container. "Firecracker: Lightweight Virtualization
for Serverless Applications" by Agache et al aims to fix this. Firecracker uses micro VMs to achieve half the boot time of leading hypervisors and maintain the same level of isolation. It does this by cutting down on unnecesary kernel components and providing a minimal manager process. Although technically this is VM technology, it has the same goals as containers and is compatible with the Docker ecosystem. Our experiments use Firecracker as part of the latest version of AWS Fargate.


\section{Key Ideas}

In order to get a good understanding of real world performance difference between VMs and containers, two different macro benchmarks were used that mimic real-world workloads: GBM-perf and RUBiS. GBM-perf is an inference workload for machine learning models that use gradient boosting, a popular ensemble technique. We chose this benchmark due to the popularity of ML inference among cloud users and its applicability to various applications. The benchmark consists of three libraries: xgboost, catboost, and lightgbm, each of which is run on 100K, 1M, and 10M records of the airline dataset. GBM-perf is mainly CPU-bound, with a small amount of sequential disk access, as well as memory access.

In order to explain the benchmark results, we used several micro benchmarks to pinpoint discrepancies for specific resources. To do this, we used Sysbench. To test CPU, Sysbench runs a brute-force prime number search. This gives a sufficient comparison given the variables that can affect CPU performance, despite the lack of advanced instructions being used. For the memory test, Sysbench allocates a memory buffer, then writes and reads blocks of data to and from it. Memory performance will impact workloads that operate on large sets of data at once, such as many common database operations, including joins, filters, and even reading several records. For disk performance, Sysbench generates a set of files, which are then read and written sequentiallty and randomly. Random read and write performance is good for workloads such as web applications, where many requests for unrelated data (from different users) occur, causing the disk to be accessed from non-contiguous locations. Sequential access is better for data processing tasks such as training an ML model or computing summary statistics on a dataset, which usually read from datasets and write intermediate parts sequentially. Using these micro benchmarks was the primary method we used to explain the macro benchmark behavior, and provide another dimension to our overall comparison.

Another measurement to aid comparison was idle resource consumption. The idea is that since VMs generally run full-fledged OSes, several background processes take up memory and a non-zero amount of CPU and disk usage. Containers, on the other hand, generally only have a single process (including any forked processes). It is possible to slim down a VM, but we still wanted to include this test to see if it's even worth it, as many VM images are not built this way. For further information about CPU, the model number was gathered for comparison.

An important aspect for cloud users when choosing infrastructure is cost. We collected pricing info for the systems we benchmarked and include this in the comparison. For EC2, the instance type determines the cost, while for Fargate, the user selected number of vCPUs and memory determine the cost. We compared using the same amount of resources for each, however EC2 has other classes of instances, some with optimized hardware, so we opted for the standard m-series ones.

Lastly, scalability is an important factor for cloud-based systems. If a system can scale well, it can meet user demand while also keeping costs to a minimum. To test this, we decided to use startup time as a metric for comparison. Based on existing work and knowledge, we assumed containers would be significantly faster in this area. However, Fargate is layered on top of ECS, which uses EC2 VMs as a base for running containers. This means for these two cloud services, Fargate containers are unable to spin up faster than EC2 instances. Additionally, we found that the Fargate startup method has other drawbacks. It will always pull the specified Docker image for every container that is started, causing long wait times for larger images. There's no option to cache the images, though ECS supports this. We also tried using the AWS container registry (ECR) to see if loading time was faster, though it turned out to be slower. We imagine this is due to Dockerhub having a more advanced CDN.


\section{Evaluation}

The evaluation process consisted of preparing the benchmarks (both micro and macro), running the benchmarks, collecting the metrics, generating charts, collecting additional information about the infrastructure, running startup tests, and finally making conclusions about which one makes more sense to use under different circumstances.

In order to collect metrics, AWS CloudWatch was used to aggregate the data, which was then downloaded to be processed by our scripts. EC2 by default doesn't log to CloudWatch, so we installed the collectd plugin, which runs as a background process on the instance. This plugin collects a wide range of metrics at a fine-grained interval (1 second), however we only used the CPU and memory metrics due to the limitations of Fargate logging. Fargate does not collect disk utilization, likely since only a small amount of ephemeral storage is provided, as containers are usually stateless. It does provide CPU and memory metrics though, but only at a 5 second granularity. Running the plugin also required attaching an IAM role to the instance to give permission to push the metrics to CloudWatch. To pull the data from CloudWatch, we used a third-party Python script called CloudWatch-dump to download the data as CSV-like files. Since the minimum polling interval supported by this script was 5 minutes, we changing it to go down to 5 seconds.

To conceptualize the data, it needed to be consolidated into charts. From the output of CloudWatch-dump, we were able to load the data into Pandas dataframes. This allowed us to format the data, take the average across several runs, then display the VM and container lines side-by-side for comparison. We also took data from other sources, such as the startup times, and put this into a custom JSON file in order to generate bar charts for easy comparison. This JSON file also included file paths to the data and chart labels.

<idle/sysbench eval>

The first step to benchmarking with GBM-perf was to setup the images. For Docker this was straightforward, since the source code included a Dockerfile to build from. The VM image was more involved, requiring backtracing the commands from the base images (rocker/tidyverse was the base image). Eventually this resulted in an installation script. After several test runs, it was discovered that at least 16 GB of memory is required to run all the GBM-perf benchmarks. A m5.xlarge instance was used for EC2, along with a gp2 (SSD) EBS volume. For Fargate, memory was set to 16 GB, along with 4 vCPUs. Fargate gets 20 GB of unspecified ephemeral storage, though from the benchmarks it seems like it's at least an SSD. More storage can be added with AWS EFS.

GBM-perf was run 5 times on each platform, each producing CPU and memory metrics. The average was taken among these and graphed in a line plot. In terms of CPU utilization, the results were very similar for both, with the VM being slightly higher. This is likely due to the VM having better sequential read speed, combined with a more powerful processor, allowing it to process more data at once, resulting in higher utilization. The VM uses a Xeon Platinum 8175M with a clockspeed of 3.1 GHz, while the container uses a Xeon E5-2686 at 2.3 GHz. The memory utilization shows a similar pattern, with the VM being slightly higher. This is likely caused by idle consumption from background processes.

<gbmperf runtimes>

<startup eval, include image sizes>


\section{Conclusion}


\section{Division of Work}



https://aws.amazon.com/blogs/containers/under-the-hood-fargate-data-plane/

\end{document}
