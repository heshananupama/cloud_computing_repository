\documentclass[11pt]{article}

\def\baselinestretch{1}
\usepackage{times}
\usepackage{titlesec}
\usepackage{fancyhdr}  % for displaying head/foot meta info 
\usepackage{pagecounting}
\usepackage{color}
\usepackage[yyyymmdd,hhmmss]{datetime}  % for using currenttime command 
\usepackage{hyperref}
\usepackage{lastpage}
\usepackage{lipsum}

\newcommand\bb[1]{\mbox{\em #1}}
\newcommand{\eat}[1]{}
\newcommand{\hsp}{\hspace*{\parindent}}
\definecolor{gray}{rgb}{0.4,0.4,0.4}


\titleformat{\section}{\vspace{- 0.5 \baselineskip}\normalfont\fontsize{12}{12}\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\vspace{- 0.5 \baselineskip}\normalfont\fontsize{11}{11}\bfseries}{\thesubsection}{1em}{}

\begin{document}

\renewcommand{\headrulewidth}{0pt} 
\renewcommand{\footrulewidth}{0pt} 
\pagestyle{fancy}
\cfoot{}
\lhead{}
\rhead{}
\rfoot{\itshape\textcolor{gray}{Page \thepage\ of \pageref{LastPage}}}
\lfoot{\itshape\textcolor{gray}{CS525T Cloud Computing Paper Review}}

%%% Fill in the paper information %%% 
\begin{center}
{\LARGE \bf Project Proposal: Cloud Comparison Between Virtual Machines and Containers} \\
{\normalsize \emph{Heshan Perera, Michael Ludwig}}\\

\end{center}

\section{Problem Statement}

% Problem statement and motivation for the problem.

Cloud is becoming more and more popular in the present world and the traditional on-premise servers are moving towards the cloud in a rapid manner. Cloud providers need to share their resources in an efficient manner mainly by utilizing resources as much as possible. While doing so, they need also need the work to be transparent to customers and give a better service in order to compete with other providers. Virtualization via containers and virtual machines are the two main methods that are used by cloud providers to provision customers' services. Most of the related work in the area has compared the two technologies from the point of view of cloud providers, i.e. which technology has the most benefit for cloud providers.

However, we didn't notice a solid comparison that is done from the role of the customer, i.e. which technology would benefit them the most. Taking this into account, in our project, our goal is to compare realistic usage of virtual machines and containers in the cloud for end users. VMs are a necessity for cloud computing because providers need to share their resources between customers, which means customers are forced to use them. However, containers have proven to have several benefits in terms of the application development lifecycle, making them appealing to cloud users. Our project aims to compare the two using different performance metrics and benchmarks. Among these, we plan to include machine learning workloads, as they are essential to modern computing, and the work we've come across hasn't taken machine learning models into account when comparing the two technologies.

\section{Related Work}

% Related work. For example, prior work that looked at similar problems or used similar techniques to what you are proposing. Focusing on the key differences between what you want to do and what have been done. This is an important aspect of the proposal that helps convince yourselves and me that spending 10 weeks on the project is worthwhile. 

"Containers and Virtual Machines at Scale: A Comparative Study" by Chaufournier et al compares VMs and containers on bare metal using five different workloads.
They concluded that containers have similar performance in most areas except disk I/O, but VMs provide better isolation.
Due to this, containers are not yet ready for multitenancy, which is why cloud providers primarily use VMs as an abstraction and don't provide a shared container service on bare metal. VMs can also do live migration, making it more suitable for the provider, since they often need to defragment their VMs.
This also means cloud end users don't need to worry about this. The paper also benchmarked containers running on VMs, and concluded there's actually a performance benefit due to the soft limits container provide. Containers also allow for faster building of images, smaller image size, and versioning of images. Another option they mentioned was a lightweight VM that has a slimmed down kernel.

We want to focus on the best approach to cloud computing for end users, and so will run our benchmarks on the cloud, which differs from this paper. We will also be comparing VMs against containers running on VMs, since cloud users don't have a bare-metal container option. We also want to use realistic workloads, which this paper only partially does. Specifically, we want to include ML training and inference tasks. The paper shows many benefits of using containers, so we want to see how they perform in the cloud and if they're a good option.

"An Updated Performance Comparison of Virtual Machines and Linux Containers" by Felter et al confirms the finding that containers have only a small performance cost versus bare metal. They use Docker instead of LXC, which is what we plan to use. Our work will differ in that we want to see if running containers on top of VMs in a cloud environment has any significant drawbacks.

\section{Solution and Design}

In order to compare VMs and container running on VMs, we plan to run a handful of benchmarking tools in two different environments and compare the results. We plan to use the following benchmarking tools: PerfKit Benchmarker [1], MLPerf [2], RUBiS [3], and YCSB [4]. The last two are also used in previous work, and represent realistic workloads. It would be useful to see how these compare to the original work, but in a cloud environment. ML workloads are very popular nowadays, and so we include MLPerf to test both training and inference performance.

For VMs, we plan to use AWS EC2, a traditional virtualized-hardware platform. We will create a VM image (AMI) for each benchmarking tool and run each on a single instance.  For containers, we will create Dockerfiles to build images to run on AWS Fargate (on top of ECS), a serverless platform for running containers. For the base operating system, we will use Ubuntu 18.04 for both the AMI and Docker image. To measure start up latencies, we plan to write a Python script that starts an instance using the AWS API and measures the time until it is ready. We will write a similar script for containers, and for cold and warm start ups for both.

\section{Plan of Work}

% Plan of work. What key questions do you need to answer? How would you implement your solution? What type of resources do you need, e.g., machines, other equipment, access to datasets ? What are the expected timeline? 

First of all, we plan to set up our AWS accounts using WPI credentials for student credits. We plan to carry out our comparison in two phases. During the initial phase, we are going to create the container images and VM images for MLperf and Rubis. Then we are going to deploy both to AWS. For running containers, we plan to use AWS Fargate, and AWS EC2 for VMs. We want to make sure to allocate resources equally. Most of the results will likely be provided by the tools themselves, but we are also planning to use CloudWatch to double check the validity with additional data. Completing these steps will make one complete cycle of our workflow and we plan to repeat the same cycle for the other two benchmarks: PerfKit and YCSB. Lastly, we plan to measure the various startup times to test the scalability. The following is a rough timeline of our work:

\begin{itemize}
\itemsep-2pt

\item 2/10 - 2/17: Setup AWS accounts with student credits; build up foundational knowledge of VM's and containers

\item 2/17 - 3/2: Create container image and VM image for MLperf (Mike); create container image and VM image for Rubis (Heshan)

\item 3/2 - 3/9: Run VMs and containers with benchmarks (figure out how to equally allocate resources)

\item 3/9 - 3/16: Gather metrics from tools and AWS CloudWatch.

\item 3/16 - 3/23: Work on mid term review; start writing scripts for measuring startup time

\item 3/23: Mid term review

\item 3/23 - 4/6: Create container image and VM image for PerfKit (Mike); Create container image and VM image for YCSB (Heshan); finish scripts for measuring startup time

\item 4/13 - 4/20: Run VMs and containers with benchmarks; gather metrics from tools and AWS Cloudwatch

\item 4/20: Presentation

\item 4/20 - 4/27: Final report

\end{itemize}

\section{Evaluation Methodology}

For results, we will use those provided by the different benchmarking tools, in addition to AWS CloudWatch metrics, to create an overall performance profile for VMs and containers. Specific metrics we want to compare are: utilization over time for each hardware resource (CPU, memory, and disk), throughput for each resource, the total time each benchmark took to run, and how long each instance took to start up. For both VMs and containers, we want to measure both cold start and warm start latency. This is important since AWS does not charge compute fees for stopped instances. Utilization and throughput will paint a picture of how efficiently each platform manages resources, while total benchmark time will determine which is best for a specific task. Lastly, the startup time will tell us how quickly each is able to scale.

\section{Division of Work}

We plan to split work by workloads, so we both get experience with both VMs and containers and associated technologies. We plan to split up the workloads as follows: Mike doing [1] and [2]; Heshan doing [3] and [4]. For each, we will do all the work including deploying, running, gathering the data, and analyzing the data. For the midterm, presentation, and final report, we will split the work evenly, each mostly focused on the benchmarks assign to us.

\hspace{16pt}

[1] https://github.com/GoogleCloudPlatform/PerfKitBenchmarker

[2] https://mlperf.org

[3] https://github.com/uillianluiz/RUBiS

[4] https://github.com/brianfrankcooper/YCSB

\end{document}
